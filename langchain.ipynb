{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import wget, requests, io\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\")\n",
    "WEAVIATE_API_KEY = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "WEAVIATE_CLUSTER_ENV = os.getenv(\"WEAVIATE_CLUSTER_ENV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI from Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"can you tell me about china\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1032\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[1;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1031\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[1;32m-> 1032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(text, stop\u001b[38;5;241m=\u001b[39m_stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:991\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 991\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    992\u001b[0m         [prompt],\n\u001b[0;32m    993\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    994\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    995\u001b[0m         tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m    996\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    997\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    998\u001b[0m     )\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m   1001\u001b[0m )\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:741\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    726\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    727\u001b[0m         )\n\u001b[0;32m    728\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    729\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    730\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    739\u001b[0m         )\n\u001b[0;32m    740\u001b[0m     ]\n\u001b[1;32m--> 741\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    742\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    743\u001b[0m     )\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:605\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    604\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    606\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:592\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    584\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 592\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    593\u001b[0m                 prompts,\n\u001b[0;32m    594\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    595\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    596\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    598\u001b[0m             )\n\u001b[0;32m    599\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    600\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    601\u001b[0m         )\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_community\\llms\\openai.py:460\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    449\u001b[0m         {\n\u001b[0;32m    450\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    457\u001b[0m         }\n\u001b[0;32m    458\u001b[0m     )\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 460\u001b[0m     response \u001b[38;5;241m=\u001b[39m completion_with_retry(\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;28mself\u001b[39m, prompt\u001b[38;5;241m=\u001b[39m_prompts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    462\u001b[0m     )\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_community\\llms\\openai.py:115\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    117\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(llm, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\openai\\resources\\completions.py:506\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    505\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_of\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mecho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\openai\\_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1199\u001b[0m     )\n\u001b[1;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\openai\\_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\openai\\_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\openai\\_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\openai\\_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\openai\\_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\openai\\_base_client.py:980\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    983\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    984\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    988\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace from Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n",
      "c:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "gemma_llm = HuggingFaceHub(repo_id=\"HuggingFaceH4/zephyr-7b-beta\", huggingfacehub_api_token=HUGGINGFACE_API_KEY)\n",
    "flan_t5_llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", huggingfacehub_api_token=HUGGINGFACE_API_KEY, model_kwargs={\"temperature\": 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Udupi is facing lot of water crisis. Could you please provide a solution for that?\n",
      "\n",
      "Udupi is facing a severe water crisis due to the failure of monsoon this year. The district has received less than 50% of the normal rainfall. The groundwater level has also depleted drastically. The situation is so bad that the district administration has declared a drought-like situation in the district.\n",
      "\n",
      "To address this issue, the government has taken several measures. The government has announced a relief package of Rs 10 crore for the\n"
     ]
    }
   ],
   "source": [
    "print(gemma_llm(\"Udupi is facing lot of water crisis. Could you please provide a solution for that?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following prompts are for use with the following template:\n"
     ]
    }
   ],
   "source": [
    "print(flan_t5_llm(\"what is prompt template?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini from Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GEMINI_API_KEY, convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Solutions to Address Water Crisis in Udupi**\n",
      "\n",
      "**1. Water Conservation Measures:**\n",
      "\n",
      "* Implement water-efficient fixtures and appliances in households, businesses, and industries.\n",
      "* Promote rainwater harvesting and storage systems for domestic and non-domestic uses.\n",
      "* Encourage drip irrigation and other water-saving techniques in agriculture.\n",
      "* Establish community water conservation awareness campaigns and incentives.\n",
      "\n",
      "**2. Water Resource Management:**\n",
      "\n",
      "* Identify and develop new water sources, such as groundwater aquifers, surface water bodies, and desalination plants.\n",
      "* Optimize existing water infrastructure by upgrading treatment plants, expanding distribution networks, and reducing water losses.\n",
      "* Implement watershed management programs to protect water bodies and recharge groundwater.\n",
      "* Enforce water conservation regulations and penalties for water misuse.\n",
      "\n",
      "**3. Alternative Water Sources:**\n",
      "\n",
      "* Utilize treated wastewater for non-potable purposes, such as irrigation and industrial cooling.\n",
      "* Explore the potential for fog harvesting and atmospheric water generation technologies.\n",
      "* Promote the use of greywater systems to reuse water from showers, sinks, and washing machines.\n",
      "\n",
      "**4. Technology and Innovation:**\n",
      "\n",
      "* Implement smart water management systems using sensors and data analytics to monitor water usage, detect leaks, and optimize distribution.\n",
      "* Develop drought-resistant crops and water-efficient landscaping techniques to reduce water demand.\n",
      "* Explore the use of artificial intelligence (AI) and machine learning (ML) to predict water demand and improve decision-making.\n",
      "\n",
      "**5. Collaboration and Partnerships:**\n",
      "\n",
      "* Establish a collaborative water management task force involving government agencies, NGOs, businesses, and community members.\n",
      "* Secure funding and support from national and international organizations to implement water conservation and infrastructure projects.\n",
      "* Share best practices and knowledge with other regions facing water challenges.\n",
      "\n",
      "**6. Public Education and Awareness:**\n",
      "\n",
      "* Launch public awareness campaigns to educate residents about the importance of water conservation and responsible water usage.\n",
      "* Establish water conservation education programs in schools and universities.\n",
      "* Encourage citizen involvement in water monitoring, conservation efforts, and reporting water-related issues.\n",
      "\n",
      "**7. Water Pricing and Incentives:**\n",
      "\n",
      "* Implement tiered water pricing systems to encourage water conservation and discourage excessive usage.\n",
      "* Provide incentives for businesses and industries that adopt water-efficient practices.\n",
      "* Offer rebates and subsidies for water conservation measures, such as rainwater harvesting systems and low-flow appliances.\n"
     ]
    }
   ],
   "source": [
    "print(google_llm.predict(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is the prime minister of India\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(input_variables=['country'], \n",
    "                                 template=\"who is the prime minister of {country}\")\n",
    "rich_template = PromptTemplate(input_variables=['country'], template='Top 5 richest people in {country}')\n",
    "p = prompt_template.format(country=\"India\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Narendra Modi\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=google_llm, prompt=prompt_template)\n",
    "response = chain.run('Indian')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the age of Narendra Modi?\n",
      "\n",
      "Narendra Modi was born on 17 September 1950. So, as of 2021, Narendra Modi's age is 70 years.\n",
      "\n",
      "Who is the current President of India?\n",
      "\n",
      "The current President of India is Ram Nath Kovind. He assumed the office on 25 July 2017.\n",
      "\n",
      "Who is the current Vice President of India?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template1 = PromptTemplate(input_variables=['PM'], \n",
    "                                  template='what is the age of {PM}')\n",
    "rich_template1 = PromptTemplate(input_variables=['person'], template='Networth of {person}')\n",
    "chain1 = LLMChain(llm=gemma_llm, prompt=prompt_template1)\n",
    "print(chain1.run(\"Narendra Modi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the age of Narendra Modi?\n",
      "\n",
      "Narendra Modi was born on 17 September 1950. So, as of 2021, Narendra Modi's age is 70 years.\n",
      "\n",
      "Who is the current President of India?\n",
      "\n",
      "The current President of India is Ram Nath Kovind. He assumed the office on 25 July 2017.\n",
      "\n",
      "Who is the current Vice President of India?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain2 = SimpleSequentialChain(chains=[chain, chain1])\n",
    "response1 = chain2.run('India')\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the age of Rishi Sunak?\n",
      "\n",
      "Rishi Sunak is a British politician who was born on May 12, 1980, in Southampton, England. As of 2021, he is 40 years old.\n",
      "\n",
      "what is the current position of Rishi Sunak in the government?\n",
      "\n",
      "Rishi Sunak is currently serving as the Chancellor of the Exchequer in the United Kingdom, a position he has held since February 2020\n"
     ]
    }
   ],
   "source": [
    "response2 = chain2.run('UK')\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Networth of Top 5 richest people in India 2019: India is the second most populous country in the world and the seventh largest country by area. It is also the worldâ€™s largest democracy and a rapidly developing country. India is a country with a rich cultural heritage and a diverse population. It is also home to some of the wealthiest people in the world. In this article, we will take a look at the top 5 richest people in India in 2019.\n",
      "\n",
      "1. Mukesh Ambani:\n",
      "\n",
      "Mukesh Ambani is the chairman and managing director of Reliance Industries Limited (RIL), an Indian conglomerate holding company. He is also the largest shareholder of the company. Ambani is the richest person in India with a net worth of $47.3 billion as of 2019. He is also ranked 13th on the Forbes list of the worldâ€™s billionaires.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rich_chain = LLMChain(llm=gemma_llm, prompt=rich_template)\n",
    "rich_chain1 = LLMChain(llm=gemma_llm, prompt=rich_template1)\n",
    "rich_chain2 = SimpleSequentialChain(chains=[rich_chain, rich_chain1])\n",
    "response3 = rich_chain2.run('India')\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Networth of Top 5 richest people in US 2018: The United States is the worldâ€™s largest economy and the most powerful country in the world. The country is home to some of the wealthiest people in the world. The US is also the birthplace of some of the worldâ€™s most successful companies like Apple, Microsoft, Google, Amazon, and Facebook.\n",
      "\n",
      "The US is also home to some of the worldâ€™s richest people. According to Forbes, the net worth of the worldâ€™s billionaires has increased by $1.1 trillion in 2017. The US is home to 585 billionaires, which is 19% of the worldâ€™s billionaire population.\n",
      "\n",
      "In this article, we will take a look at the top 5 richest people in the US in 2018.\n",
      "\n",
      "1. Jeff Bezos: Net Worth $112 Billion\n",
      "\n",
      "Jeff Bezos is\n"
     ]
    }
   ],
   "source": [
    "response3 = rich_chain2.run('US')\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget -q https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip\n",
    "!unzip -q new_articles.zip -d new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\zipfile.py:1266\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1266\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1268\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\zipfile.py:1333\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[1;32m-> 1333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1335\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip\")\n",
    "z = ZipFile(io.BytesIO(r.content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "data = PyPDFDirectoryLoader(\"SAG\")\n",
    "data1 = data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Content  \\n  \\n Revision history  ................................ ................................ ................................ .........................  8 \\nAdded sec 13.6.10 Mandatory fields based on Operational Categorization Tier 1  ................................ . 11 \\n Introduction  ................................ ................................ ................................ .............................  14 \\n Limitations  14', metadata={'source': 'SAG\\\\SystemAdminGuide.pdf', 'page': 0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1213"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.034477267414331436,\n",
       " 0.031023206189274788,\n",
       " 0.006734929047524929,\n",
       " 0.026108982041478157,\n",
       " -0.03936203196644783,\n",
       " -0.16030243039131165,\n",
       " 0.06692398339509964,\n",
       " -0.006441489793360233,\n",
       " -0.04745049402117729,\n",
       " 0.014758839271962643,\n",
       " 0.07087527960538864,\n",
       " 0.05552763119339943,\n",
       " 0.01919335499405861,\n",
       " -0.026251323521137238,\n",
       " -0.010109569877386093,\n",
       " -0.02694045566022396,\n",
       " 0.022307392209768295,\n",
       " -0.02222662791609764,\n",
       " -0.14969263970851898,\n",
       " -0.017493031919002533,\n",
       " 0.007676294539123774,\n",
       " 0.054352253675460815,\n",
       " 0.0032544503919780254,\n",
       " 0.031725917011499405,\n",
       " -0.08462144434452057,\n",
       " -0.029405998066067696,\n",
       " 0.051595620810985565,\n",
       " 0.048124048858881,\n",
       " -0.00331486901268363,\n",
       " -0.058279212564229965,\n",
       " 0.041969265788793564,\n",
       " 0.022210726514458656,\n",
       " 0.12818878889083862,\n",
       " -0.02233896590769291,\n",
       " -0.011656217277050018,\n",
       " 0.06292834132909775,\n",
       " -0.032876402139663696,\n",
       " -0.09122605621814728,\n",
       " -0.03117532841861248,\n",
       " 0.05269954353570938,\n",
       " 0.047034818679094315,\n",
       " -0.08420311659574509,\n",
       " -0.030056139454245567,\n",
       " -0.020744847133755684,\n",
       " 0.00951785035431385,\n",
       " -0.0037217815406620502,\n",
       " 0.007343289442360401,\n",
       " 0.039324335753917694,\n",
       " 0.09327401965856552,\n",
       " -0.0037885704077780247,\n",
       " -0.05274210125207901,\n",
       " -0.05805816128849983,\n",
       " -0.0068643661215901375,\n",
       " 0.005283265374600887,\n",
       " 0.08289296180009842,\n",
       " 0.019362766295671463,\n",
       " 0.006284523755311966,\n",
       " -0.010330776683986187,\n",
       " 0.009032350033521652,\n",
       " -0.03768376260995865,\n",
       " -0.04520610347390175,\n",
       " 0.024016328155994415,\n",
       " -0.006944178603589535,\n",
       " 0.013491666875779629,\n",
       " 0.10005496442317963,\n",
       " -0.07168389856815338,\n",
       " -0.0216950885951519,\n",
       " 0.0316183976829052,\n",
       " -0.051634643226861954,\n",
       " -0.08224772661924362,\n",
       " -0.06569328159093857,\n",
       " -0.009895360097289085,\n",
       " 0.005816400051116943,\n",
       " 0.07355451583862305,\n",
       " -0.034050293266773224,\n",
       " 0.024886099621653557,\n",
       " 0.014488058164715767,\n",
       " 0.02645733207464218,\n",
       " 0.009656756184995174,\n",
       " 0.030217278748750687,\n",
       " 0.05280396714806557,\n",
       " -0.07535987347364426,\n",
       " 0.009897195734083652,\n",
       " 0.02983679249882698,\n",
       " 0.01755557581782341,\n",
       " 0.023091968148946762,\n",
       " 0.001933887368068099,\n",
       " 0.0014001816743984818,\n",
       " -0.04717600718140602,\n",
       " -0.011194308288395405,\n",
       " -0.11420147120952606,\n",
       " -0.019811974838376045,\n",
       " 0.040266212075948715,\n",
       " 0.002192983403801918,\n",
       " -0.07979220151901245,\n",
       " -0.025382336229085922,\n",
       " 0.09448299556970596,\n",
       " -0.028981046751141548,\n",
       " -0.145002543926239,\n",
       " 0.23097741603851318,\n",
       " 0.02773120254278183,\n",
       " 0.03211143612861633,\n",
       " 0.031065043061971664,\n",
       " 0.0428328700363636,\n",
       " 0.06423782557249069,\n",
       " 0.03216315805912018,\n",
       " -0.004876766353845596,\n",
       " 0.0556994192302227,\n",
       " -0.03753232955932617,\n",
       " -0.021505534648895264,\n",
       " -0.028342677280306816,\n",
       " -0.028846904635429382,\n",
       " 0.038353074342012405,\n",
       " -0.01746865175664425,\n",
       " 0.052485328167676926,\n",
       " -0.07487606257200241,\n",
       " -0.031259745359420776,\n",
       " 0.021841589361429214,\n",
       " -0.03989570587873459,\n",
       " -0.008587072603404522,\n",
       " 0.02695656754076481,\n",
       " -0.04849553853273392,\n",
       " 0.01146987359970808,\n",
       " 0.02961820736527443,\n",
       " -0.020572206005454063,\n",
       " 0.013103843666613102,\n",
       " 0.02883346751332283,\n",
       " -3.194198717480234e-33,\n",
       " 0.06478208303451538,\n",
       " -0.018130162730813026,\n",
       " 0.05178993567824364,\n",
       " 0.12198273092508316,\n",
       " 0.02878018654882908,\n",
       " 0.008721956983208656,\n",
       " -0.07052122056484222,\n",
       " -0.01690729148685932,\n",
       " 0.04073967784643173,\n",
       " 0.04211616888642311,\n",
       " 0.025447174906730652,\n",
       " 0.0357462540268898,\n",
       " -0.04914470762014389,\n",
       " 0.0021290022414177656,\n",
       " -0.015546590089797974,\n",
       " 0.05073055997490883,\n",
       " -0.0481853112578392,\n",
       " 0.03588065132498741,\n",
       " -0.0040669748559594154,\n",
       " 0.10172472149133682,\n",
       " -0.05597000569105148,\n",
       " -0.010681062936782837,\n",
       " 0.01123578567057848,\n",
       " 0.09068655967712402,\n",
       " 0.004234429448843002,\n",
       " 0.035138651728630066,\n",
       " -0.00970285665243864,\n",
       " -0.09386516362428665,\n",
       " 0.0928555503487587,\n",
       " 0.008004932664334774,\n",
       " -0.007705426309257746,\n",
       " -0.05208666995167732,\n",
       " -0.01258796639740467,\n",
       " 0.0032669196370989084,\n",
       " 0.006013510283082724,\n",
       " 0.007581599522382021,\n",
       " 0.010517144575715065,\n",
       " -0.08634555339813232,\n",
       " -0.06987880915403366,\n",
       " -0.002533914986997843,\n",
       " -0.09097655117511749,\n",
       " 0.04688732698559761,\n",
       " 0.0520765483379364,\n",
       " 0.007193876430392265,\n",
       " 0.010903693735599518,\n",
       " -0.005229538306593895,\n",
       " 0.01393736433237791,\n",
       " 0.021968355402350426,\n",
       " 0.03420860692858696,\n",
       " 0.060224711894989014,\n",
       " 0.00011668332444969565,\n",
       " 0.014731977134943008,\n",
       " -0.07008923590183258,\n",
       " 0.028499027714133263,\n",
       " -0.027601705864071846,\n",
       " 0.01076838281005621,\n",
       " 0.03483099117875099,\n",
       " -0.022487888112664223,\n",
       " 0.009769042022526264,\n",
       " 0.07722783833742142,\n",
       " 0.021588364616036415,\n",
       " 0.11495617032051086,\n",
       " -0.0680011734366417,\n",
       " 0.023760948330163956,\n",
       " -0.015983937308192253,\n",
       " -0.01782693713903427,\n",
       " 0.06439489126205444,\n",
       " 0.032025717198848724,\n",
       " 0.05027022212743759,\n",
       " -0.005913727451115847,\n",
       " -0.03370805084705353,\n",
       " 0.017840318381786346,\n",
       " 0.016573376953601837,\n",
       " 0.0632965937256813,\n",
       " 0.03467715159058571,\n",
       " 0.046473462134599686,\n",
       " 0.09790615737438202,\n",
       " -0.006635511759668589,\n",
       " 0.025207115337252617,\n",
       " -0.07798829674720764,\n",
       " 0.01692643202841282,\n",
       " -0.0009457895648665726,\n",
       " 0.022471901029348373,\n",
       " -0.038253217935562134,\n",
       " 0.09570480138063431,\n",
       " -0.005350861698389053,\n",
       " 0.01046909298747778,\n",
       " -0.11524059623479843,\n",
       " -0.013262546621263027,\n",
       " -0.01070947851985693,\n",
       " -0.08311733603477478,\n",
       " 0.07327356934547424,\n",
       " 0.0493922233581543,\n",
       " -0.008994347415864468,\n",
       " -0.09584552049636841,\n",
       " 3.36614782706661e-33,\n",
       " 0.12493177503347397,\n",
       " 0.019349701702594757,\n",
       " -0.05822572857141495,\n",
       " -0.03598823398351669,\n",
       " -0.05074669048190117,\n",
       " -0.045662377029657364,\n",
       " -0.08260341733694077,\n",
       " 0.14819476008415222,\n",
       " -0.08842121064662933,\n",
       " 0.06027442589402199,\n",
       " 0.05103018507361412,\n",
       " 0.010303172282874584,\n",
       " 0.14121422171592712,\n",
       " 0.030813805758953094,\n",
       " 0.061033133417367935,\n",
       " -0.05285126343369484,\n",
       " 0.13664892315864563,\n",
       " 0.009189915843307972,\n",
       " -0.01732519455254078,\n",
       " -0.012848632410168648,\n",
       " -0.007995259016752243,\n",
       " -0.050980012863874435,\n",
       " -0.05235058441758156,\n",
       " 0.007593023590743542,\n",
       " -0.015166297554969788,\n",
       " 0.01696031726896763,\n",
       " 0.021270547062158585,\n",
       " 0.020558040589094162,\n",
       " -0.12002810090780258,\n",
       " 0.01446179673075676,\n",
       " 0.026759929955005646,\n",
       " 0.025330713018774986,\n",
       " -0.04275466129183769,\n",
       " 0.006768449675291777,\n",
       " -0.014458579011261463,\n",
       " 0.04526194557547569,\n",
       " -0.09147652983665466,\n",
       " -0.019439082592725754,\n",
       " -0.017833532765507698,\n",
       " -0.054910123348236084,\n",
       " -0.05264108628034592,\n",
       " -0.010459048673510551,\n",
       " -0.05201604217290878,\n",
       " 0.02089199237525463,\n",
       " -0.07997027784585953,\n",
       " -0.01211128756403923,\n",
       " -0.057731445878744125,\n",
       " 0.023178208619356155,\n",
       " -0.008031727746129036,\n",
       " -0.025989286601543427,\n",
       " -0.07995668798685074,\n",
       " -0.020728835836052895,\n",
       " 0.0488177128136158,\n",
       " -0.02038915455341339,\n",
       " -0.04917658120393753,\n",
       " 0.014159676618874073,\n",
       " -0.06362204998731613,\n",
       " -0.007807373534888029,\n",
       " 0.016431551426649094,\n",
       " -0.025682521983981133,\n",
       " 0.013381141237914562,\n",
       " 0.026248741894960403,\n",
       " 0.009978409856557846,\n",
       " 0.06322891265153885,\n",
       " 0.002672187052667141,\n",
       " -0.006582782603800297,\n",
       " 0.016631923615932465,\n",
       " 0.03236640617251396,\n",
       " 0.03794242814183235,\n",
       " -0.03637608513236046,\n",
       " -0.006910961586982012,\n",
       " 0.00015965897182468325,\n",
       " -0.0016335470136255026,\n",
       " -0.027278143912553787,\n",
       " -0.02803812548518181,\n",
       " 0.049681439995765686,\n",
       " -0.02886725217103958,\n",
       " -0.002418083604425192,\n",
       " 0.014774886891245842,\n",
       " 0.0097645940259099,\n",
       " 0.005797603167593479,\n",
       " 0.013486184179782867,\n",
       " 0.005567966494709253,\n",
       " 0.03722710162401199,\n",
       " 0.0072324699722230434,\n",
       " 0.040156275033950806,\n",
       " 0.08150329440832138,\n",
       " 0.0719916969537735,\n",
       " -0.013056163676083088,\n",
       " -0.04288205876946449,\n",
       " -0.01101123820990324,\n",
       " 0.004897788166999817,\n",
       " -0.009229675866663456,\n",
       " 0.03519143536686897,\n",
       " -0.05103500187397003,\n",
       " -1.571437557856825e-08,\n",
       " -0.08862444758415222,\n",
       " 0.023909326642751694,\n",
       " -0.016238754615187645,\n",
       " 0.031700499355793,\n",
       " 0.02728426828980446,\n",
       " 0.052468784153461456,\n",
       " -0.04707099497318268,\n",
       " -0.05884745717048645,\n",
       " -0.06320817023515701,\n",
       " 0.04088853672146797,\n",
       " 0.04982801526784897,\n",
       " 0.10655171424150467,\n",
       " -0.07450231909751892,\n",
       " -0.012495404109358788,\n",
       " 0.01837068609893322,\n",
       " 0.039474062621593475,\n",
       " -0.024797867983579636,\n",
       " 0.014516236260533333,\n",
       " -0.03706920146942139,\n",
       " 0.020015694200992584,\n",
       " -4.860065018874593e-05,\n",
       " 0.00986657477915287,\n",
       " 0.024838773533701897,\n",
       " -0.05245812609791756,\n",
       " 0.029314259067177773,\n",
       " -0.08719190955162048,\n",
       " -0.014499680139124393,\n",
       " 0.026019100099802017,\n",
       " -0.01874633878469467,\n",
       " -0.07620517909526825,\n",
       " 0.035043299198150635,\n",
       " 0.10363949835300446,\n",
       " -0.028050484135746956,\n",
       " 0.012718163430690765,\n",
       " -0.07632549852132797,\n",
       " -0.018652379512786865,\n",
       " 0.02497672103345394,\n",
       " 0.08144532889127731,\n",
       " 0.06875884532928467,\n",
       " -0.06405662000179291,\n",
       " -0.08389385044574738,\n",
       " 0.0613623671233654,\n",
       " -0.03354554623365402,\n",
       " -0.10615334659814835,\n",
       " -0.04008055850863457,\n",
       " 0.032530203461647034,\n",
       " 0.07662484794855118,\n",
       " -0.07301618903875351,\n",
       " 0.00033756825723685324,\n",
       " -0.040871620178222656,\n",
       " -0.07578851282596588,\n",
       " 0.02752765268087387,\n",
       " 0.07462538778781891,\n",
       " 0.01771732047200203,\n",
       " 0.0912184938788414,\n",
       " 0.11022018641233444,\n",
       " 0.0005697841988876462,\n",
       " 0.05146336182951927,\n",
       " -0.014551290310919285,\n",
       " 0.03323199227452278,\n",
       " 0.023792298510670662,\n",
       " -0.022889820858836174,\n",
       " 0.03893749788403511,\n",
       " 0.030206842347979546]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = embedding.embed_query(\"Hello world\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the code to push vector embeddings to Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'db'\n",
    "\n",
    "vector_db = Chroma.from_documents(documents=data1, embedding=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the code to retrive similar vector embeddings for a particular query from Chroma DB (Semantic Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=' Ericsson Internal  \\nSYSTEM ADMINISTRATION GUIDE   \\n24 (334) \\nPrepared (Subject resp)  No. \\nZNIAISD Aditi Singh  1543 -AOT 101 5177 Uen  \\nApproved (Document resp)  Checked  Date  Rev Reference  \\n  2020 -11-04 PX4  \\n \\n3.1.1.6.1  Sending CI Unavailability details to OneFM for Alarm Suppression  \\nI. CI unavailability added to CRs will be sent along with the CIs to OneFM for Alarm \\nSuppression. Thi s feature is configurable in the company form . \\nâ€¢ â€œCIâ€ â€“ Only CI  & Cloud CI  will be sent to OneFM  \\nâ€¢ â€œCI Unavailabillityâ€ - Only CI marked unavailable will be sent to OneFM along \\nwith the unavailability start time and end time.  \\nâ€¢ â€œBoth CI and Unavailabilityâ€  â€“ Both CI and CI marked unavailable will be sent \\nto OneFM  \\n \\nII. Implementation In Progress status will be sent to OneFM for Alarm Suppression. \\nThis feature is configurable in ITSM . Based on configuration the CR details will be \\nsent to OneFM for Alarm Suppression Either in Scheduled or Implementation In \\nProgress.  \\n \\n', metadata={'page': 23, 'source': 'SAG\\\\SystemAdminGuide.pdf'}),\n",
       " Document(page_content=' Ericsson Internal  \\nSYSTEM ADMINISTRATION GUIDE   \\n171 (334) \\nPrepared (Subject resp)  No. \\nZNIAISD Aditi Singh  1543 -AOT 101 5177 Uen  \\nApproved (Document resp)  Checked  Date  Rev Reference  \\n  2020 -11-04 PX4  \\n \\n \\n \\n5.5.74   Alarm Suppress Possibility configuration  \\nPreviously if OneFM interface  configuration  is enabled, the configuration is applicable for \\nboth Incident module and change module. Which means to create tickets in ITSM from \\nOneFM as well as to stop ticket creation from OneFM by using alarm suppression \\nfunctionality for the Node CI and Site CI sen t towards OneFM via Change module.  \\nNow as part of this Alarm Suppress possibility configuration, we have separate \\nconfiguration to restrict Site CI and Node CI to send to OneFM for Alarm suppression. To \\nenable this, we need to select Alarm Suppress Possibi lity and Suppress Alarm from CRQ.  \\nc) Alarm Suppress Possibility  â†’In Scheduled Status - which means in Scheduled \\nstatus of the CR ticket the Node CI and Site CI details will be sent to OneFM for \\nalarm suppression, so that ticket creation in ITSM will be stopp ed during this time. \\nThis is existing functionality itself. We are adding more conditions to send Site \\nCI/Node CI towards OneFM, along with this configuration, OneFM interface \\nconfiguration also need to be enabled.  \\nd) Alarm Suppress Possibility â†’ In IIP Statu s â€“ Currently not in scope.  \\ne) OneFM CR interface â†’ Irrespective of CR â€“ which means all Node CI/Site CI \\nattached in the CR ticket will be sent to OneFM for Alarm Suppression.  \\nf) OneFM CR interface â†’ Per CR â€“ which means NOC user will have option to \\nchoose speci fically send Site CI/Node CI or not to send any CI towards OneFM \\nfor alarm suppression  \\n', metadata={'page': 170, 'source': 'SAG\\\\SystemAdminGuide.pdf'}),\n",
       " Document(page_content=' Ericsson Internal  \\nSYSTEM ADMINISTRATION GUIDE   \\n11 (334) \\nPrepared (Subject resp)  No. \\nZNIAISD Aditi Singh  1543 -AOT 101 5177 Uen  \\nApproved (Document resp)  Checked  Date  Rev Reference  \\n  2020 -11-04 PX4  \\n \\n04-12-2020  PA51  Added Section 5.2.3 Worklog View \\nAccess configurable  Abhishek Sirohi  \\n10-12-2020  PA52  Added 5.2.10 Mandatory Event Start \\nTime  Shreesha Narasimha \\nMurthy  \\n25-01-2020  PA53  Added Sec 3.1.1.6.1 Sending CI \\nUnavailability details to OneFM for \\nAlarm Suppression  Saranya Murali  \\n8-02-2020  PA54  Added Sec 5.6.9  for BPMS interface  Jaynat Dusane  \\n19-02-2021  PA55  Added Sec 5.2.17 EAI Inventory \\nEnhancement for ITObjects.  Prasad M K  \\n21-02-2021  PA56  Added Sec 3.1.1.6.2 Event End Time \\nModification.  Prasad M K  \\n18-03-2021  \\n PA57  \\n Added sec 13.6.10 Mandatory fields \\nbased on Operational Categorization \\nTier 1  Manju Amarnath \\nPalleda  \\n05-04-2021  PA58  3.1.1.6.3  Resolution Tier1 and \\nResolution Tier2 updates for WFM \\nCancellation.  Prasad M K  \\n15-04-2021  PA59  Updated Section â€“ 5.6 MATE â€“ ITSM -\\nWFM integration  Shreesha Narasimha \\nMurthy  \\n18-05-2021  PA60  Added 5.2.24 Mandatory Fields based \\non Parameter Values  Arun Kaushik  \\n27-05-2021  PA61  Added Auto approval configuration \\nunder section 5.6.7  Jayant Dusane  \\n06-07-2021  PA62  Added Associating Secondary CI to \\nIncident under section 5.2.25  Chandramani N.K.  \\n21-09-2021  PA63  Added Section 5.2.7 Restrict Duplicate \\nVendor Ticket ID  \\nAdded Section 5.2.8  Configuration for \\nRequested Dates Mandatory during \\nChange Create.  Prasad M K  \\n29-09-2021  PA64  5.2.28  Assigned Group Change based \\non Site Access details  \\n5.2.29 Populating Enterprise customer \\nname based on Node for problem and \\nknown Error  Chandramani N.K.  \\n10-10-2021  PA65  5.2.35 WO/task deduplication in ITSM  Laxman Sanigarapu  ', metadata={'page': 10, 'source': 'SAG\\\\SystemAdminGuide.pdf'}),\n",
       " Document(page_content=' Ericsson Internal  \\nSYSTEM ADMINISTRATION GUIDE   \\n25 (334) \\nPrepared (Subject resp)  No. \\nZNIAISD Aditi Singh  1543 -AOT 101 5177 Uen  \\nApproved (Document resp)  Checked  Date  Rev Reference  \\n  2020 -11-04 PX4  \\n \\nIII. When the Scheduled CR moves to any back status or to pending, the CR details \\nwill be sent to OneFM to stop the alarm suppression . \\n3.1.1.6.2             Event End Time Modification  \\nOverwriting Event End Time on the Incident form by WFM, whenever WFM \\ncloses the task is been restricted based on the flag.  \\nIf the Flag is set to Yes, Event End Time from WFM Task closure will be \\noverwritten with the W FM Task Closure Time.  \\nIf the Flag is set to No, Event End Time from WFM Task closure will not be \\noverwritten with WFM Task Closure Time.    \\nBelow is the Configuration that needs to be enabled on Company form.  \\nOverwrite Event End Time with WFM.  \\n \\n3.1.1.6.3                           Overwrite Cancellation updates for Resolution Tier1 and \\nResolution Tier2 fields from WFM interface.  \\nResolution Category Tier1 and Resolution Category Tier2 values for WFM \\nCancellation will be set as below : - \\nâ€¢ Resolution Tier 1 = Auto Alarm Ce asedâ€  \\nâ€¢ Resolution Tier 2 =  â€œCleared by OneFMâ€  \\nIf the Flag is set to Yes,  \\nâ€¢ Resolution Tier 1 = Auto Alarm Ceasedâ€  \\nâ€¢ Resolution Tier 2 = â€œCleared by OneFMâ€  \\n', metadata={'page': 24, 'source': 'SAG\\\\SystemAdminGuide.pdf'})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_db.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"Sending CI Unavailability details to OneFM for Alarm Suppression\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the code to integrate LLM with Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      " Ericsson Internal  \n",
      "SYSTEM ADMINISTRATION GUIDE   \n",
      "81 (334) \n",
      "Prepared (Subject resp)  No. \n",
      "ZNIAISD Aditi Singh  1543 -AOT 101 5177 Uen  \n",
      "Approved (Document resp)  Checked  Date  Rev Reference  \n",
      "  2020 -11-04 PX4  \n",
      " \n",
      " \n",
      "c) Select or enter the operational categorization tiers. After you create an \n",
      "operational category with a particular tier 1 and tier 2, you can select those tiers \n",
      "when creating additional operational categories. For example, if you create an \n",
      "operational category for Add > User > Account, you might first enter all three \n",
      "categories, but when you create an operational category for Add > User > Web \n",
      "Page, you can select both  Add and User.  \n",
      "d) Select  Enabled  for the status.  \n",
      "e) Select whether the category is available for the company that you are configuring, \n",
      "or whether it is available f or all companies.  \n",
      "f) Click  Add. When you save the operational category, it is automatically related to \n",
      "the Global company. This makes the operational category available on other \n",
      "forms, such as the Incident form, for all operating and customer companies. The \n",
      "product category is also available to all BMC Remedy ITSM applications.  \n",
      "g) To add more operational categories, repeat steps c through f.  \n",
      "h) When you are finished adding operational categories, click  Close.  \n",
      "\n",
      "\n",
      " Ericsson Internal  \n",
      "SYSTEM ADMINISTRATION GUIDE   \n",
      "144 (334) \n",
      "Prepared (Subject resp)  No. \n",
      "ZNIAISD Aditi Singh  1543 -AOT 101 5177 Uen  \n",
      "Approved (Document resp)  Checked  Date  Rev Reference  \n",
      "  2020 -11-04 PX4  \n",
      " \n",
      "b) Operational Categorization Tier 2  \n",
      "c) Operational Categorization Tier 3  \n",
      "d) Weightage  \n",
      "Configuration Steps:  \n",
      "â€¢ Open Eric:TaskConfiguration form  \n",
      "â€¢ Select configuration type as â€œActivityâ€  \n",
      "â€¢ Select the required customer name in company dropdown.  \n",
      "â€¢ Click on Activity tab  \n",
      "â€¢ Give Activity and Node Type Values  \n",
      "â€¢  Select Opcat Tier1, Opcat Tier2 and Opcat Tier 3  \n",
      "â€¢ Give High weightage  \n",
      "â€¢ Save the record  \n",
      " \n",
      " \n",
      "Note:  \n",
      "\n",
      "\n",
      " Ericsson Internal  \n",
      "SYSTEM ADMINISTRATION GUIDE   \n",
      "80 (334) \n",
      "Prepared (Subject resp)  No. \n",
      "ZNIAISD Aditi Singh  1543 -AOT 101 5177 Uen  \n",
      "Approved (Document resp)  Checked  Date  Rev Reference  \n",
      "  2020 -11-04 PX4  \n",
      " \n",
      "f) (Optional)  Enter a product name.  \n",
      "g) If you specify a product name, select a manufacturer or click  New to add a \n",
      "manufacturer. If you click  New:  \n",
      "â€¢ In the New Manufacturer dialog box, enter a company.  \n",
      "â€¢ In the  Status  field, select  Enabled . \n",
      "â€¢ Click  Save . \n",
      "h) In the Product Category dialog box, select  Enabled  for the status.  \n",
      "i) Leave the Origin default as Custom.  \n",
      "j) If this product definition is a product suite definition, select  Yes. \n",
      "k) Select whether this product definition is just for the current company or is to be \n",
      "used by all companies configured.  \n",
      "l) Click  Add. \n",
      "The product category is auto matically related to the selected company and, is \n",
      "available on other forms in the selected applications, such as the Incident form.  \n",
      "m) To add more product categories, repeat steps  c to l. \n",
      "n) Click  Close.  \n",
      " To create Operational Categories  \n",
      "a) On the  Standard C onfiguration  tab of the Application Administration Console, \n",
      "select the correct company.  \n",
      "b) Click the  Create  link next to Operational Category. The Operational Catalog \n",
      "dialog box appears.  \n",
      " \n",
      "\n",
      " Ericsson Internal  \n",
      "SYSTEM ADMINISTRATION GUIDE   \n",
      "83 (334) \n",
      "Prepared (Subject resp)  No. \n",
      "ZNIAISD Aditi Singh  1543 -AOT 101 5177 Uen  \n",
      "Approved (Document resp)  Checked  Date  Rev Reference  \n",
      "  2020 -11-04 PX4  \n",
      " \n",
      "f) In the Routing Order area, specify when this assignment entry should be used for \n",
      "the automated assignment.  \n",
      "Company  Select the location of the incident or request. If this routing \n",
      "applies to all companies, select  Global . \n",
      "Operational \n",
      "Categorization  You can route assignments by operational services.  \n",
      "Product \n",
      "Categorization  You can route assignments by product categorization.  \n",
      "g) Click  Add. \n",
      "h) Repeat step c through step g to add more assignment routings.  \n",
      "   Incident T emplate  Enhancement  to include Service_Affecting field . \n",
      " In the Application Administration Console, select the Custom Configuration tab and \n",
      "expand the Incident Management branch.  \n",
      " Select Template > Template.  \n",
      " Click Create.  \n",
      " In the Template Name field on the Incident Template form, type a brief descriptive \n",
      "name for the template.  \n",
      " Set the Status field.  \n",
      " Select or create the appropriate template categorizations for Tier 1, Tier 2, and Tier 3. \n",
      " To create a template categorization, type the category name in the Template \n",
      "Category field. When you save the template, the category name is also saved and \n",
      "added to the field's selection list, where it is available to select when you create the \n",
      "next tem plate.  \n",
      " Enter or modify the appropriate settings on the template tabs.  \n",
      " Select Service_Affecting field value as â€œNo/Yesâ€ . \n",
      "\n",
      "Question: Steps to create Operational Categories\n",
      "Helpful Answer: a) On the Standard C configuration tab of the Application Administration Console, select the correct company. b) Click the Create link next to Operational Category. The Operational Catalog dialog box appears.\n"
     ]
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=gemma_llm, retriever=retriever, chain_type=\"stuff\")\n",
    "response = qa.run(\"Steps to create Operational Categories\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader, CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"PDFs\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "text_chunks = text_splitter.split_documents(data)\n",
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='CS391R: Robot Learning (Fall 2021)\\nYou Only Look Once (YOLO): Unified, Real-Time Object Detection\\n1Presenter: Shivang SinghSept 2nd, 2021', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)2Problem Addressed: Object Detectionâ–Object detection is the problem of both locating ANDclassifying objects â–Goal of YOLO algorithm is to do object detection both fast ANDwith high accuracy\\nâ€œDeep Learning for Vision Systemsâ€ (Elgendy)Object Detection vs Classification', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)3Importance of Object Detection for Roboticsâ–Visual modality is very powerfulâ–Humans are able to detect objects and do perception using just this modality in real time (not needing radar) â–If we want responsive robot systems that work in real time (without specialized sensors) almost real time vision based object detection can help greatly\\nVision based vs LIDAR (self driving)\\nTesla Investor Day Presentation', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)4Previous Object Detection ParadigmThis pipeline was used in nearly all SOTA Object Detection prior: \\nStep 1: Scan the image to generate candidate bounding boxes\\nImage ClassifierLabel + confidencehat -0.92racket -0.2ball -0.23Step 2: Run the bounding box through a classifierStep 3: Conduct post-processing (filtering out redundant bounding boxes)Diagram developed by presenter', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)5Key Insightsâ–A separate model for generating bounding boxes and for classification (more complicated model pipeline)â–Need to run classification many times (expensive computation)â–Looks at limited part of the image (lacks contextual information for detection) Previous Approachesâ–A single neural network for localization and for classification (less complicated pipeline)â–Need to inference only once (efficient computation)â–Looks at the entire image each time', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='Previous Approachesâ–A single neural network for localization and for classification (less complicated pipeline)â–Need to inference only once (efficient computation)â–Looks at the entire image each time leading to less false positives (has contextual information for detection) YOLO algorithm', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)6Formal Problem Settingâ–Given an image generate bounding boxes, one for each detectable object in image â–For each bounding box, output 5 predictions: x, y, w, h, confidence. Also output classâ–x, y (coordinates for center of bounding box)â–w,h (width and height)â–confidence (probability bounding box has object)â–class (classification of object in bounding box)', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)7Related Work-R-CNN or Region Based Convolutional Network (Girshick et al. 2014):-Used the sliding window approach from earlier, with Selective Search, a smarter way to select candidates (which means there is less computation)-Still feeds a limited part of the image to the classifier-Drawbacks: Large pipeline, slow, too many false positives-Fast and Faster R-CNN: -Optimize parts of the pipeline described earlier -Drawbacks: loses accuracy-Deep Multibox (Szegedy', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='to the classifier-Drawbacks: Large pipeline, slow, too many false positives-Fast and Faster R-CNN: -Optimize parts of the pipeline described earlier -Drawbacks: loses accuracy-Deep Multibox (Szegedy et. al 2014):-Train a CNN to find areas of interest-Drawbacks: Doesnâ€™t address classification only localization', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)8Related Work-MultiGrasp (Redmon et. al 2014)-Similar to YOLO-A much simpler task (only needs to predict object not multiple objects)', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)9YOLO overviewâ–First, image is split into a SxS gridâ–For each grid square, generate B bounding boxesâ–For each bounding box, there are 5 predictions: x, y, w, h, confidence \\nS = 3, B = 2', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)\\n10YOLO Trainingâ–YOLO is a regression algorithm. What is X? What is Y?â–X is simple, just an image width (in pixels) * height (in pixels) * RGB valuesâ–Y is a tensor of size S * S * (B * 5 + C)â–B*5 + C term represents the predictions + class predicted distribution for a grid blockFor each grid block, we have a vector like this. For this example B is 2 and C is 2\\nGT label example:', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 9}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)11YOLO Architecture-Now that we know the input and output, we can discuss the model-We are given 448 by 448 by 3 as our input.-Implementation uses 7 convolution layers -Paper parameters: S = 7, B = 2, C = 20-Output is S*S*(5B+C) = 7*7*(5*2+20) = 7*7*30', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 10}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)12YOLO Prediction\\nâ–We then use the output to make final detectionsâ–Use a threshold to filter out bounding boxes with low P(Object)â–In order to know the class for the bounding box compute score take argmax over the distribution Pr(Class|Object) for the grid the bounding boxâ€™s center is in', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 11}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)13Non-maximal suppressionâ–Most of the time objects fall in one grid, however it is still possible to get redundant boxes (rare case as object must be close to multiple grid cells for this to happen)â–Discard bounding box with high overlap (keeping the bounding box with highest confidence)â–Adds 2-3% on final mAP score', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 12}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)14YOLO Objective Functionâ–For YOLO, we need to minimize the following lossâ–Sum squared error is used\\nCoordinate Loss: Minimize the difference between x,y,w,h pred and x,y,w,h ground truth.  ONLY IF object exists in grid box and if bounding box is resp for predClass loss, minimize loss between true class of object in grid box Confidence Loss: Loss based on confidence ONLY IF there is object No Object Loss based on confidence if there is no object', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 13}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)15Experimental Setupâ–Authors compare YOLO against the previous work described above on PASCAL VOC 2007, and VOC 2012 as well as out of domain art dataset â–Correct if IOU metric above .5 and class is correctâ–Use two performance metrics:âž¢mAP score: mean average precisionâž¢FPS: frames per secondâ–Add FAST YOLO: which has less parameters', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 14}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)16Experimental Results\\nâ–Baseline YOLO outperform real time detectors by large amountâ–Do better than most less than real time as well', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 15}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)17\\nExperimental Results', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 16}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)18Exper-Makes far less background errors (less likely to predict false positives on background)-IOU is VERY small with any ground truth label-But far more localization errors-Correct class, IOU is somewhat small\\nExperimental Results -Error Analysis\\nBackground errorLocalization error', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 17}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)19â–Ran YOLO + competitors (trained on natural images) on art â–Does well on artistic datasets where more having global context greatly helpsExperimental Results -Out of Domain', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 18}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)20Discussion of Resultsâ–Pro: YOLO is a lot faster than the other algorithms for image detectionâ–Pro: YOLOâ€™s use of global information rather than only local information allows it to understand contextual information when doing object detectionâž¢Does better in domains such as artwork due to thisâ–Con: YOLO lagged behind the SOTA models in object detectionâž¢This is attributed to making many localization errors and unable to detect small object', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 19}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)21Critique / Limitations / Open Issues â–Performance lags behind SOTA â–Requires data to be labeled with bounding boxes, hard to collect for many classesâž¢Previous work could generalize better since it used image classifierâž¢2014 COCO dataset (very large dataset) addressed this somewhatâ–Regarding experiments: number of classes predicted is very limitedâž¢Not convinced that YOLO v1 is generalizable â–Confidence output of YOLO not confidence of class but P(Object),', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 20}),\n",
       " Document(page_content='this somewhatâ–Regarding experiments: number of classes predicted is very limitedâž¢Not convinced that YOLO v1 is generalizable â–Confidence output of YOLO not confidence of class but P(Object), lowers interpretabilityâ–Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since only B boxes can be predicted on an SxS gridâ–Since the architecture only predicts boxes, this might make it less useful for irregular shapes', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 20}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)22Future Work for Paper / Readingâ–One extension of this work would be to look at image segmentation and see if the insights carry overâ—‹YOLOACT (Boyla et al 2019): Real time image segmentation â–YOLO has been upgraded 2 times â—‹Solves a lot of issues relating to detecting small objects, generalizability, and localization\\nYOLOACT example', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 21}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)23Extended Readingsâ–YOLO v2 (https://arxiv.org/abs/1506.02640) (extends on the work greatly)(Redmond et al 2016)âž¢Deals with the generalizability problem, has 9000 classesâž¢Class probability distribution per bounding box, not per gridâž¢High resolution classifier (finetune on high resolution)âž¢Batch normâž¢Trained on MSCOCO (released after YOLO v1 paper)â–YOLO v3 (https://arxiv.org/abs/1804.02767) âž¢â€œIncremental Improvementâ€âž¢Uses independent logistic classifiers for', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 22}),\n",
       " Document(page_content='on high resolution)âž¢Batch normâž¢Trained on MSCOCO (released after YOLO v1 paper)â–YOLO v3 (https://arxiv.org/abs/1804.02767) âž¢â€œIncremental Improvementâ€âž¢Uses independent logistic classifiers for classâ– Allows for more specificity in classes', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 22}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)24Summaryâ–Object detection is the problem of detecting multiple objects in an imageâ–Almost real time object detection can make highly responsive robot systems without complex sensorsâ–Prior work relies on a large architecture with numerous parts to optimizeâ–YOLO proposes a unified architecture, which does all the tasks in one model and by one inference over the entire imageâ–They show enormous speed improvement and show that they can beat most other prior work in', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 23}),\n",
       " Document(page_content='a unified architecture, which does all the tasks in one model and by one inference over the entire imageâ–They show enormous speed improvement and show that they can beat most other prior work in terms of mAPs', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 23})]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='CS391R: Robot Learning (Fall 2021)2Problem Addressed: Object Detectionâ–Object detection is the problem of both locating ANDclassifying objects â–Goal of YOLO algorithm is to do object detection both fast ANDwith high accuracy\\nâ€œDeep Learning for Vision Systemsâ€ (Elgendy)Object Detection vs Classification', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)3Importance of Object Detection for Roboticsâ–Visual modality is very powerfulâ–Humans are able to detect objects and do perception using just this modality in real time (not needing radar) â–If we want responsive robot systems that work in real time (without specialized sensors) almost real time vision based object detection can help greatly\\nVision based vs LIDAR (self driving)\\nTesla Investor Day Presentation', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)4Previous Object Detection ParadigmThis pipeline was used in nearly all SOTA Object Detection prior: \\nStep 1: Scan the image to generate candidate bounding boxes\\nImage ClassifierLabel + confidencehat -0.92racket -0.2ball -0.23Step 2: Run the bounding box through a classifierStep 3: Conduct post-processing (filtering out redundant bounding boxes)Diagram developed by presenter', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[1], text_chunks[2], text_chunks[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CS391R: Robot Learning (Fall 2021)2Problem Addressed: Object Detectionâ–Object detection is the problem of both locating ANDclassifying objects â–Goal of YOLO algorithm is to do object detection both fast ANDwith high accuracy\\nâ€œDeep Learning for Vision Systemsâ€ (Elgendy)Object Detection vs Classification'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emebddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.034477267414331436,\n",
       " 0.031023206189274788,\n",
       " 0.006734929047524929,\n",
       " 0.026108982041478157,\n",
       " -0.03936203196644783,\n",
       " -0.16030243039131165,\n",
       " 0.06692398339509964,\n",
       " -0.006441489793360233,\n",
       " -0.04745049402117729,\n",
       " 0.014758839271962643,\n",
       " 0.07087527960538864,\n",
       " 0.05552763119339943,\n",
       " 0.01919335499405861,\n",
       " -0.026251323521137238,\n",
       " -0.010109569877386093,\n",
       " -0.02694045566022396,\n",
       " 0.022307392209768295,\n",
       " -0.02222662791609764,\n",
       " -0.14969263970851898,\n",
       " -0.017493031919002533,\n",
       " 0.007676294539123774,\n",
       " 0.054352253675460815,\n",
       " 0.0032544503919780254,\n",
       " 0.031725917011499405,\n",
       " -0.08462144434452057,\n",
       " -0.029405998066067696,\n",
       " 0.051595620810985565,\n",
       " 0.048124048858881,\n",
       " -0.00331486901268363,\n",
       " -0.058279212564229965,\n",
       " 0.041969265788793564,\n",
       " 0.022210726514458656,\n",
       " 0.12818878889083862,\n",
       " -0.02233896590769291,\n",
       " -0.011656217277050018,\n",
       " 0.06292834132909775,\n",
       " -0.032876402139663696,\n",
       " -0.09122605621814728,\n",
       " -0.03117532841861248,\n",
       " 0.05269954353570938,\n",
       " 0.047034818679094315,\n",
       " -0.08420311659574509,\n",
       " -0.030056139454245567,\n",
       " -0.020744847133755684,\n",
       " 0.00951785035431385,\n",
       " -0.0037217815406620502,\n",
       " 0.007343289442360401,\n",
       " 0.039324335753917694,\n",
       " 0.09327401965856552,\n",
       " -0.0037885704077780247,\n",
       " -0.05274210125207901,\n",
       " -0.05805816128849983,\n",
       " -0.0068643661215901375,\n",
       " 0.005283265374600887,\n",
       " 0.08289296180009842,\n",
       " 0.019362766295671463,\n",
       " 0.006284523755311966,\n",
       " -0.010330776683986187,\n",
       " 0.009032350033521652,\n",
       " -0.03768376260995865,\n",
       " -0.04520610347390175,\n",
       " 0.024016328155994415,\n",
       " -0.006944178603589535,\n",
       " 0.013491666875779629,\n",
       " 0.10005496442317963,\n",
       " -0.07168389856815338,\n",
       " -0.0216950885951519,\n",
       " 0.0316183976829052,\n",
       " -0.051634643226861954,\n",
       " -0.08224772661924362,\n",
       " -0.06569328159093857,\n",
       " -0.009895360097289085,\n",
       " 0.005816400051116943,\n",
       " 0.07355451583862305,\n",
       " -0.034050293266773224,\n",
       " 0.024886099621653557,\n",
       " 0.014488058164715767,\n",
       " 0.02645733207464218,\n",
       " 0.009656756184995174,\n",
       " 0.030217278748750687,\n",
       " 0.05280396714806557,\n",
       " -0.07535987347364426,\n",
       " 0.009897195734083652,\n",
       " 0.02983679249882698,\n",
       " 0.01755557581782341,\n",
       " 0.023091968148946762,\n",
       " 0.001933887368068099,\n",
       " 0.0014001816743984818,\n",
       " -0.04717600718140602,\n",
       " -0.011194308288395405,\n",
       " -0.11420147120952606,\n",
       " -0.019811974838376045,\n",
       " 0.040266212075948715,\n",
       " 0.002192983403801918,\n",
       " -0.07979220151901245,\n",
       " -0.025382336229085922,\n",
       " 0.09448299556970596,\n",
       " -0.028981046751141548,\n",
       " -0.145002543926239,\n",
       " 0.23097741603851318,\n",
       " 0.02773120254278183,\n",
       " 0.03211143612861633,\n",
       " 0.031065043061971664,\n",
       " 0.0428328700363636,\n",
       " 0.06423782557249069,\n",
       " 0.03216315805912018,\n",
       " -0.004876766353845596,\n",
       " 0.0556994192302227,\n",
       " -0.03753232955932617,\n",
       " -0.021505534648895264,\n",
       " -0.028342677280306816,\n",
       " -0.028846904635429382,\n",
       " 0.038353074342012405,\n",
       " -0.01746865175664425,\n",
       " 0.052485328167676926,\n",
       " -0.07487606257200241,\n",
       " -0.031259745359420776,\n",
       " 0.021841589361429214,\n",
       " -0.03989570587873459,\n",
       " -0.008587072603404522,\n",
       " 0.02695656754076481,\n",
       " -0.04849553853273392,\n",
       " 0.01146987359970808,\n",
       " 0.02961820736527443,\n",
       " -0.020572206005454063,\n",
       " 0.013103843666613102,\n",
       " 0.02883346751332283,\n",
       " -3.194198717480234e-33,\n",
       " 0.06478208303451538,\n",
       " -0.018130162730813026,\n",
       " 0.05178993567824364,\n",
       " 0.12198273092508316,\n",
       " 0.02878018654882908,\n",
       " 0.008721956983208656,\n",
       " -0.07052122056484222,\n",
       " -0.01690729148685932,\n",
       " 0.04073967784643173,\n",
       " 0.04211616888642311,\n",
       " 0.025447174906730652,\n",
       " 0.0357462540268898,\n",
       " -0.04914470762014389,\n",
       " 0.0021290022414177656,\n",
       " -0.015546590089797974,\n",
       " 0.05073055997490883,\n",
       " -0.0481853112578392,\n",
       " 0.03588065132498741,\n",
       " -0.0040669748559594154,\n",
       " 0.10172472149133682,\n",
       " -0.05597000569105148,\n",
       " -0.010681062936782837,\n",
       " 0.01123578567057848,\n",
       " 0.09068655967712402,\n",
       " 0.004234429448843002,\n",
       " 0.035138651728630066,\n",
       " -0.00970285665243864,\n",
       " -0.09386516362428665,\n",
       " 0.0928555503487587,\n",
       " 0.008004932664334774,\n",
       " -0.007705426309257746,\n",
       " -0.05208666995167732,\n",
       " -0.01258796639740467,\n",
       " 0.0032669196370989084,\n",
       " 0.006013510283082724,\n",
       " 0.007581599522382021,\n",
       " 0.010517144575715065,\n",
       " -0.08634555339813232,\n",
       " -0.06987880915403366,\n",
       " -0.002533914986997843,\n",
       " -0.09097655117511749,\n",
       " 0.04688732698559761,\n",
       " 0.0520765483379364,\n",
       " 0.007193876430392265,\n",
       " 0.010903693735599518,\n",
       " -0.005229538306593895,\n",
       " 0.01393736433237791,\n",
       " 0.021968355402350426,\n",
       " 0.03420860692858696,\n",
       " 0.060224711894989014,\n",
       " 0.00011668332444969565,\n",
       " 0.014731977134943008,\n",
       " -0.07008923590183258,\n",
       " 0.028499027714133263,\n",
       " -0.027601705864071846,\n",
       " 0.01076838281005621,\n",
       " 0.03483099117875099,\n",
       " -0.022487888112664223,\n",
       " 0.009769042022526264,\n",
       " 0.07722783833742142,\n",
       " 0.021588364616036415,\n",
       " 0.11495617032051086,\n",
       " -0.0680011734366417,\n",
       " 0.023760948330163956,\n",
       " -0.015983937308192253,\n",
       " -0.01782693713903427,\n",
       " 0.06439489126205444,\n",
       " 0.032025717198848724,\n",
       " 0.05027022212743759,\n",
       " -0.005913727451115847,\n",
       " -0.03370805084705353,\n",
       " 0.017840318381786346,\n",
       " 0.016573376953601837,\n",
       " 0.0632965937256813,\n",
       " 0.03467715159058571,\n",
       " 0.046473462134599686,\n",
       " 0.09790615737438202,\n",
       " -0.006635511759668589,\n",
       " 0.025207115337252617,\n",
       " -0.07798829674720764,\n",
       " 0.01692643202841282,\n",
       " -0.0009457895648665726,\n",
       " 0.022471901029348373,\n",
       " -0.038253217935562134,\n",
       " 0.09570480138063431,\n",
       " -0.005350861698389053,\n",
       " 0.01046909298747778,\n",
       " -0.11524059623479843,\n",
       " -0.013262546621263027,\n",
       " -0.01070947851985693,\n",
       " -0.08311733603477478,\n",
       " 0.07327356934547424,\n",
       " 0.0493922233581543,\n",
       " -0.008994347415864468,\n",
       " -0.09584552049636841,\n",
       " 3.36614782706661e-33,\n",
       " 0.12493177503347397,\n",
       " 0.019349701702594757,\n",
       " -0.05822572857141495,\n",
       " -0.03598823398351669,\n",
       " -0.05074669048190117,\n",
       " -0.045662377029657364,\n",
       " -0.08260341733694077,\n",
       " 0.14819476008415222,\n",
       " -0.08842121064662933,\n",
       " 0.06027442589402199,\n",
       " 0.05103018507361412,\n",
       " 0.010303172282874584,\n",
       " 0.14121422171592712,\n",
       " 0.030813805758953094,\n",
       " 0.061033133417367935,\n",
       " -0.05285126343369484,\n",
       " 0.13664892315864563,\n",
       " 0.009189915843307972,\n",
       " -0.01732519455254078,\n",
       " -0.012848632410168648,\n",
       " -0.007995259016752243,\n",
       " -0.050980012863874435,\n",
       " -0.05235058441758156,\n",
       " 0.007593023590743542,\n",
       " -0.015166297554969788,\n",
       " 0.01696031726896763,\n",
       " 0.021270547062158585,\n",
       " 0.020558040589094162,\n",
       " -0.12002810090780258,\n",
       " 0.01446179673075676,\n",
       " 0.026759929955005646,\n",
       " 0.025330713018774986,\n",
       " -0.04275466129183769,\n",
       " 0.006768449675291777,\n",
       " -0.014458579011261463,\n",
       " 0.04526194557547569,\n",
       " -0.09147652983665466,\n",
       " -0.019439082592725754,\n",
       " -0.017833532765507698,\n",
       " -0.054910123348236084,\n",
       " -0.05264108628034592,\n",
       " -0.010459048673510551,\n",
       " -0.05201604217290878,\n",
       " 0.02089199237525463,\n",
       " -0.07997027784585953,\n",
       " -0.01211128756403923,\n",
       " -0.057731445878744125,\n",
       " 0.023178208619356155,\n",
       " -0.008031727746129036,\n",
       " -0.025989286601543427,\n",
       " -0.07995668798685074,\n",
       " -0.020728835836052895,\n",
       " 0.0488177128136158,\n",
       " -0.02038915455341339,\n",
       " -0.04917658120393753,\n",
       " 0.014159676618874073,\n",
       " -0.06362204998731613,\n",
       " -0.007807373534888029,\n",
       " 0.016431551426649094,\n",
       " -0.025682521983981133,\n",
       " 0.013381141237914562,\n",
       " 0.026248741894960403,\n",
       " 0.009978409856557846,\n",
       " 0.06322891265153885,\n",
       " 0.002672187052667141,\n",
       " -0.006582782603800297,\n",
       " 0.016631923615932465,\n",
       " 0.03236640617251396,\n",
       " 0.03794242814183235,\n",
       " -0.03637608513236046,\n",
       " -0.006910961586982012,\n",
       " 0.00015965897182468325,\n",
       " -0.0016335470136255026,\n",
       " -0.027278143912553787,\n",
       " -0.02803812548518181,\n",
       " 0.049681439995765686,\n",
       " -0.02886725217103958,\n",
       " -0.002418083604425192,\n",
       " 0.014774886891245842,\n",
       " 0.0097645940259099,\n",
       " 0.005797603167593479,\n",
       " 0.013486184179782867,\n",
       " 0.005567966494709253,\n",
       " 0.03722710162401199,\n",
       " 0.0072324699722230434,\n",
       " 0.040156275033950806,\n",
       " 0.08150329440832138,\n",
       " 0.0719916969537735,\n",
       " -0.013056163676083088,\n",
       " -0.04288205876946449,\n",
       " -0.01101123820990324,\n",
       " 0.004897788166999817,\n",
       " -0.009229675866663456,\n",
       " 0.03519143536686897,\n",
       " -0.05103500187397003,\n",
       " -1.571437557856825e-08,\n",
       " -0.08862444758415222,\n",
       " 0.023909326642751694,\n",
       " -0.016238754615187645,\n",
       " 0.031700499355793,\n",
       " 0.02728426828980446,\n",
       " 0.052468784153461456,\n",
       " -0.04707099497318268,\n",
       " -0.05884745717048645,\n",
       " -0.06320817023515701,\n",
       " 0.04088853672146797,\n",
       " 0.04982801526784897,\n",
       " 0.10655171424150467,\n",
       " -0.07450231909751892,\n",
       " -0.012495404109358788,\n",
       " 0.01837068609893322,\n",
       " 0.039474062621593475,\n",
       " -0.024797867983579636,\n",
       " 0.014516236260533333,\n",
       " -0.03706920146942139,\n",
       " 0.020015694200992584,\n",
       " -4.860065018874593e-05,\n",
       " 0.00986657477915287,\n",
       " 0.024838773533701897,\n",
       " -0.05245812609791756,\n",
       " 0.029314259067177773,\n",
       " -0.08719190955162048,\n",
       " -0.014499680139124393,\n",
       " 0.026019100099802017,\n",
       " -0.01874633878469467,\n",
       " -0.07620517909526825,\n",
       " 0.035043299198150635,\n",
       " 0.10363949835300446,\n",
       " -0.028050484135746956,\n",
       " 0.012718163430690765,\n",
       " -0.07632549852132797,\n",
       " -0.018652379512786865,\n",
       " 0.02497672103345394,\n",
       " 0.08144532889127731,\n",
       " 0.06875884532928467,\n",
       " -0.06405662000179291,\n",
       " -0.08389385044574738,\n",
       " 0.0613623671233654,\n",
       " -0.03354554623365402,\n",
       " -0.10615334659814835,\n",
       " -0.04008055850863457,\n",
       " 0.032530203461647034,\n",
       " 0.07662484794855118,\n",
       " -0.07301618903875351,\n",
       " 0.00033756825723685324,\n",
       " -0.040871620178222656,\n",
       " -0.07578851282596588,\n",
       " 0.02752765268087387,\n",
       " 0.07462538778781891,\n",
       " 0.01771732047200203,\n",
       " 0.0912184938788414,\n",
       " 0.11022018641233444,\n",
       " 0.0005697841988876462,\n",
       " 0.05146336182951927,\n",
       " -0.014551290310919285,\n",
       " 0.03323199227452278,\n",
       " 0.023792298510670662,\n",
       " -0.022889820858836174,\n",
       " 0.03893749788403511,\n",
       " 0.030206842347979546]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_result = embeddings.embed_query(\"Hello World\")\n",
    "q_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Piencone DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY,\n",
    "              environment=PINECONE_API_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the code to push the vector embeddings to Pinecone DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "index = \"myindex\"\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in text_chunks], embeddings, index_name=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the code to retrieve the embeddings from Pinecone DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Pinecone.from_existing_index(index_name=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='this somewhatâ–Regarding experiments: number of classes predicted is very limitedâž¢Not convinced that YOLO v1 is generalizable â–Confidence output of YOLO not confidence of class but P(Object), lowers interpretabilityâ–Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since only B boxes can be predicted on an SxS gridâ–Since the architecture only predicts boxes, this might make it less useful for irregular shapes'),\n",
       " Document(page_content='Previous Approachesâ–A single neural network for localization and for classification (less complicated pipeline)â–Need to inference only once (efficient computation)â–Looks at the entire image each time leading to less false positives (has contextual information for detection) YOLO algorithm'),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)20Discussion of Resultsâ–Pro: YOLO is a lot faster than the other algorithms for image detectionâ–Pro: YOLOâ€™s use of global information rather than only local information allows it to understand contextual information when doing object detectionâž¢Does better in domains such as artwork due to thisâ–Con: YOLO lagged behind the SOTA models in object detectionâž¢This is attributed to making many localization errors and unable to detect small object'),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)2Problem Addressed: Object Detectionâ–Object detection is the problem of both locating ANDclassifying objects â–Goal of YOLO algorithm is to do object detection both fast ANDwith high accuracy\\nâ€œDeep Learning for Vision Systemsâ€ (Elgendy)Object Detection vs Classification')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Yolo\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the code to integrate LLM with Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "this somewhatâ–Regarding experiments: number of classes predicted is very limitedâž¢Not convinced that YOLO v1 is generalizable â–Confidence output of YOLO not confidence of class but P(Object), lowers interpretabilityâ–Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since only B boxes can be predicted on an SxS gridâ–Since the architecture only predicts boxes, this might make it less useful for irregular shapes\n",
      "\n",
      "Previous Approachesâ–A single neural network for localization and for classification (less complicated pipeline)â–Need to inference only once (efficient computation)â–Looks at the entire image each time leading to less false positives (has contextual information for detection) YOLO algorithm\n",
      "\n",
      "CS391R: Robot Learning (Fall 2021)20Discussion of Resultsâ–Pro: YOLO is a lot faster than the other algorithms for image detectionâ–Pro: YOLOâ€™s use of global information rather than only local information allows it to understand contextual information when doing object detectionâž¢Does better in domains such as artwork due to thisâ–Con: YOLO lagged behind the SOTA models in object detectionâž¢This is attributed to making many localization errors and unable to detect small object\n",
      "\n",
      "CS391R: Robot Learning (Fall 2021)2Problem Addressed: Object Detectionâ–Object detection is the problem of both locating ANDclassifying objects â–Goal of YOLO algorithm is to do object detection both fast ANDwith high accuracy\n",
      "â€œDeep Learning for Vision Systemsâ€ (Elgendy)Object Detection vs Classification\n",
      "\n",
      "Question: What is Yolo\n",
      "Helpful Answer: Yolo, or You Only Look Once, is a real-time object detection algorithm that aims to achieve both fast and accurate object detection. It addresses the problem of object detection by predicting bounding boxes and class probabilities directly from an image, rather than scanning the image multiple times as in traditional two-stage detectors. Yolo's use of global information and grid-based architecture allows it to understand contextual information and detect objects with high accuracy, while its speed makes it a\n"
     ]
    }
   ],
   "source": [
    "retriever = docsearch.as_retriever()\n",
    "qa = RetrievalQA.from_chain_type(llm=gemma_llm, chain_type=\"stuff\", retriever=retriever)\n",
    "print(qa.run(\"What is Yolo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Yolo?\n",
      "\n",
      "Yolo is a deep learning algorithm that is used for object detection and classification in real-time applications. It stands for \"You Only Look Once,\" which refers to the fact that the algorithm processes the entire image in a single pass, making it faster and more efficient than other object detection algorithms that require multiple passes.\n",
      "\n",
      "How does Yolo work?\n",
      "\n",
      "Yolo uses a convolutional neural network (CNN) to extract features from an input image. The CNN is trained\n"
     ]
    }
   ],
   "source": [
    "print(gemma_llm(\"What is Yolo?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "this somewhatâ–Regarding experiments: number of classes predicted is very limitedâž¢Not convinced that YOLO v1 is generalizable â–Confidence output of YOLO not confidence of class but P(Object), lowers interpretabilityâ–Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since only B boxes can be predicted on an SxS gridâ–Since the architecture only predicts boxes, this might make it less useful for irregular shapes\n",
    "\n",
    "Previous Approachesâ–A single neural network for localization and for classification (less complicated pipeline)â–Need to inference only once (efficient computation)â–Looks at the entire image each time leading to less false positives (has contextual information for detection) YOLO algorithm\n",
    "\n",
    "CS391R: Robot Learning (Fall 2021)20Discussion of Resultsâ–Pro: YOLO is a lot faster than the other algorithms for image detectionâ–Pro: YOLOâ€™s use of global information rather than only local information allows it to understand contextual information when doing object detectionâž¢Does better in domains such as artwork due to thisâ–Con: YOLO lagged behind the SOTA models in object detectionâž¢This is attributed to making many localization errors and unable to detect small object\n",
    "\n",
    "CS391R: Robot Learning (Fall 2021)2Problem Addressed: Object Detectionâ–Object detection is the problem of both locating ANDclassifying objects â–Goal of YOLO algorithm is to do object detection both fast ANDwith high accuracy\n",
    "â€œDeep Learning for Vision Systemsâ€ (Elgendy)Object Detection vs Classification\n",
    "\n",
    "Question: What is Yolo?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "this somewhatâ–Regarding experiments: number of classes predicted is very limitedâž¢Not convinced that YOLO v1 is generalizable â–Confidence output of YOLO not confidence of class but P(Object), lowers interpretabilityâ–Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since only B boxes can be predicted on an SxS gridâ–Since the architecture only predicts boxes, this might make it less useful for irregular shapes\n",
      "\n",
      "Previous Approachesâ–A single neural network for localization and for classification (less complicated pipeline)â–Need to inference only once (efficient computation)â–Looks at the entire image each time leading to less false positives (has contextual information for detection) YOLO algorithm\n",
      "\n",
      "CS391R: Robot Learning (Fall 2021)20Discussion of Resultsâ–Pro: YOLO is a lot faster than the other algorithms for image detectionâ–Pro: YOLOâ€™s use of global information rather than only local information allows it to understand contextual information when doing object detectionâž¢Does better in domains such as artwork due to thisâ–Con: YOLO lagged behind the SOTA models in object detectionâž¢This is attributed to making many localization errors and unable to detect small object\n",
      "\n",
      "CS391R: Robot Learning (Fall 2021)2Problem Addressed: Object Detectionâ–Object detection is the problem of both locating ANDclassifying objects â–Goal of YOLO algorithm is to do object detection both fast ANDwith high accuracy\n",
      "â€œDeep Learning for Vision Systemsâ€ (Elgendy)Object Detection vs Classification\n",
      "\n",
      "Question: What is Yolo? How does it differ from previous approaches to object detection and classification?\n"
     ]
    }
   ],
   "source": [
    "print(gemma_llm(query1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"PDFs\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_vector_db = FAISS.from_documents(text_chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='this somewhatâ–Regarding experiments: number of classes predicted is very limitedâž¢Not convinced that YOLO v1 is generalizable â–Confidence output of YOLO not confidence of class but P(Object), lowers interpretabilityâ–Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since only B boxes can be predicted on an SxS gridâ–Since the architecture only predicts boxes, this might make it less useful for irregular shapes', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 20}),\n",
       " Document(page_content='Previous Approachesâ–A single neural network for localization and for classification (less complicated pipeline)â–Need to inference only once (efficient computation)â–Looks at the entire image each time leading to less false positives (has contextual information for detection) YOLO algorithm', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)20Discussion of Resultsâ–Pro: YOLO is a lot faster than the other algorithms for image detectionâ–Pro: YOLOâ€™s use of global information rather than only local information allows it to understand contextual information when doing object detectionâž¢Does better in domains such as artwork due to thisâ–Con: YOLO lagged behind the SOTA models in object detectionâž¢This is attributed to making many localization errors and unable to detect small object', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 19}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)2Problem Addressed: Object Detectionâ–Object detection is the problem of both locating ANDclassifying objects â–Goal of YOLO algorithm is to do object detection both fast ANDwith high accuracy\\nâ€œDeep Learning for Vision Systemsâ€ (Elgendy)Object Detection vs Classification', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Yolo?\"\n",
    "faiss_vector_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='this somewhatâ–Regarding experiments: number of classes predicted is very limitedâž¢Not convinced that YOLO v1 is generalizable â–Confidence output of YOLO not confidence of class but P(Object), lowers interpretabilityâ–Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since only B boxes can be predicted on an SxS gridâ–Since the architecture only predicts boxes, this might make it less useful for irregular shapes', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 20}),\n",
       " Document(page_content='Previous Approachesâ–A single neural network for localization and for classification (less complicated pipeline)â–Need to inference only once (efficient computation)â–Looks at the entire image each time leading to less false positives (has contextual information for detection) YOLO algorithm', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='CS391R: Robot Learning (Fall 2021)20Discussion of Resultsâ–Pro: YOLO is a lot faster than the other algorithms for image detectionâ–Pro: YOLOâ€™s use of global information rather than only local information allows it to understand contextual information when doing object detectionâž¢Does better in domains such as artwork due to thisâ–Con: YOLO lagged behind the SOTA models in object detectionâž¢This is attributed to making many localization errors and unable to detect small object', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_vector_db.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "this somewhatâ–Regarding experiments: number of classes predicted is very limitedâž¢Not convinced that YOLO v1 is generalizable â–Confidence output of YOLO not confidence of class but P(Object), lowers interpretabilityâ–Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since only B boxes can be predicted on an SxS gridâ–Since the architecture only predicts boxes, this might make it less useful for irregular shapes\n",
      "\n",
      "Previous Approachesâ–A single neural network for localization and for classification (less complicated pipeline)â–Need to inference only once (efficient computation)â–Looks at the entire image each time leading to less false positives (has contextual information for detection) YOLO algorithm\n",
      "\n",
      "CS391R: Robot Learning (Fall 2021)20Discussion of Resultsâ–Pro: YOLO is a lot faster than the other algorithms for image detectionâ–Pro: YOLOâ€™s use of global information rather than only local information allows it to understand contextual information when doing object detectionâž¢Does better in domains such as artwork due to thisâ–Con: YOLO lagged behind the SOTA models in object detectionâž¢This is attributed to making many localization errors and unable to detect small object\n",
      "\n",
      "CS391R: Robot Learning (Fall 2021)2Problem Addressed: Object Detectionâ–Object detection is the problem of both locating ANDclassifying objects â–Goal of YOLO algorithm is to do object detection both fast ANDwith high accuracy\n",
      "â€œDeep Learning for Vision Systemsâ€ (Elgendy)Object Detection vs Classification\n",
      "\n",
      "Question: What is Yolo?\n",
      "Helpful Answer: Yolo, or You Only Look Once, is an algorithm for object detection in images that aims to achieve both speed and accuracy in object detection. It differs from previous approaches that separate localization and classification into two stages, as Yolo predicts both bounding boxes and class probabilities simultaneously in a single pass through the image. Yolo's use of global information and grid-based architecture allows it to understand contextual information and perform better in certain domains, although it may lag behind other\n"
     ]
    }
   ],
   "source": [
    "faiss_qa = RetrievalQA.from_chain_type(llm=gemma_llm, retriever=faiss_vector_db.as_retriever())\n",
    "print(faiss_qa.run(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaviate DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'field_validator' from 'pydantic' (C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\pydantic\\__init__.cp39-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweaviate\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\weaviate\\__init__.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PackageNotFoundError:\n\u001b[0;32m     12\u001b[0m     __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client, WeaviateClient\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     connect_to_custom,\n\u001b[0;32m     17\u001b[0m     connect_to_embedded,\n\u001b[0;32m     18\u001b[0m     connect_to_local,\n\u001b[0;32m     19\u001b[0m     connect_to_wcs,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     auth,\n\u001b[0;32m     24\u001b[0m     backup,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     types,\n\u001b[0;32m     38\u001b[0m )\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\weaviate\\client.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m HttpxError\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m RequestsConnectionError\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweaviate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackup\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _Backup\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweaviate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _GQLEntryReturnType, _RawGQLReturn\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AuthCredentials\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\weaviate\\backup\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mModule for backup/restore operations\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackup\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackupStorage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Backup\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackupStorage\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\weaviate\\backup\\backup.py:12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m RequestsConnectionError\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweaviate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Connection, ConnectionV4\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweaviate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     BackupFailedException,\n\u001b[0;32m     15\u001b[0m     EmptyResponseException,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweaviate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _capitalize_first_letter, _decode_json_response_dict\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\weaviate\\connect\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mModule communication to a Weaviate instance. Used to connect to\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mWeaviate and run REST requests.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConnectionParams, ProtocolParams\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Connection\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConnectionV4\n",
      "File \u001b[1;32mc:\\Ravi\\GenerativeAI\\Langchain\\venv\\lib\\site-packages\\weaviate\\connect\\base.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgrpc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Channel, ssl_channel_credentials\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgrpc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Channel \u001b[38;5;28;01mas\u001b[39;00m AsyncChannel  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, field_validator, model_validator\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweaviate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Proxies\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweaviate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NUMBER\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'field_validator' from 'pydantic' (C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\pydantic\\__init__.cp39-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "import weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'field_validator' from 'pydantic' (C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\pydantic\\__init__.cp39-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m field_validator\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'field_validator' from 'pydantic' (C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\pydantic\\__init__.cp39-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "from pydantic import field_validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boktiar@ineuron.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
